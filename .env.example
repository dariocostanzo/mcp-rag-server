# Ollama Configuration
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_MODEL="mistral" # Change this to your preferred Ollama model (e.g., llama2, llama3)
                       # Make sure you have pulled this model using `ollama pull mistral`